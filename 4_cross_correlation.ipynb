{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Black to format the code in this notebook. If you want to contribute, use the following to format code cells upon running them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jupyter_black\n",
    "\n",
    "# jupyter_black.load(\n",
    "#     lab=False,\n",
    "#     line_length=120,\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why\n",
    "\n",
    "What are you looking at now? Typically in the year 2023, you'd be looking at an a computer display that has about 2 million to 8 million pixels, with 3 color channels each. In the multilayer perceptrons (MLPs) that we've looked at before, taking one of these images as input would require, for example, 3 * 3840 * 2160 = 24883200 parameters in the first layer. This would work, but it's mostly a waste of parameters due to certain properties of natural images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this image in the context of identifying the primary object in the image. \n",
    "<div>\n",
    "<img src=\"data/cat.jpg\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "([Source](https://commons.wikimedia.org/wiki/File:Orange_tabby_cat_sitting_on_fallen_leaves-Hisashi-01A.jpg))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that task, this image contains a lot if unnecessary and redundant information. For example, while most of the image is of leaves on the ground, most humans would say that this image is of a cat. Also, we don't really need much information about color or texture to do that. Let's do some image processing to reduce unnecessary information and see whether we can still say it's a cat."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I obtained this image as a JPEG file, we can use the file size as a rough proxy for the amount of information in the image. As is, the image size in bytes is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(\"data/cat.jpg\").stat().st_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's change the image to grayscale. We will use `torchvision`, a computer vision tools package for PyTorch, to read the image file and put its data into a `Tensor` shaped (number of color channels, height, width). Then, we make a grayscale image by averaging across the color channel dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torchvision.io.read_image(\"data/cat.jpg\")\n",
    "grayscaled_image = image.type(torch.float32).mean(0, keepdim=True).type(torch.uint8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function helps us look at images in `Tensor`s. The type hint `torch.Tensor | tuple[torch.Tensor, ...]` means that images can be a Tensor or a tuple of any length with elements of type Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images: torch.Tensor | tuple[torch.Tensor, ...]):\n",
    "    if not isinstance(images, tuple):\n",
    "        images = (images,)\n",
    "    _, axs = plt.subplots(ncols=len(images), squeeze=False)\n",
    "    for i, image in enumerate(images):\n",
    "        image = image.detach()\n",
    "        if image.shape[0] == 3:\n",
    "            axs[0, i].imshow(image.permute(1, 2, 0).numpy())\n",
    "        else:\n",
    "            axs[0, i].imshow(image[0].numpy(), cmap=\"gray\")\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(grayscaled_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how big this would be if saved in JPEG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = pathlib.Path(\"temp\")\n",
    "temp_folder.mkdir(exist_ok=True)\n",
    "\n",
    "torchvision.io.write_jpeg(grayscaled_image, str(temp_folder / \"cat_grayscale.jpg\"), quality=90)\n",
    "(temp_folder / \"cat_grayscale.jpg\").stat().st_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pretty high quality settings, it's already much smaller.\n",
    "\n",
    "Another thing we could do is lower the number of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscaled_image.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this resolution is unnecessarily high for the task, we'll downsample it to a quarter the resolution by keeping every fourth pixel. The syntax `::4` is shorthand for `0:(end+1):4`, where `end` is `grayscaled_image.shape[1]` or ...`[2]`, which means to start at index 0, end at the last index, and take steps of size 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_image = grayscaled_image[:, ::4, ::4]\n",
    "downsampled_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(downsampled_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still looks like a cat to me."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we could do is just keep some edges, say the vertical edges. We can do that by applying a filter to the image using cross-correlation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-correlation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional layer, despite its name, is often a **cross-correlation**, followed by an addition.\n",
    "\n",
    "The cross-correlation of vectors $\\mathbf{w}$ and $\\mathbf{x}$ is $\\mathbf{w} \\star \\mathbf{x} = \\mathbf{z} \\in \\mathbb{R}^{|\\mathbf{x}|-|\\mathbf{w}|+1}$ whose each element\n",
    "$\\mathbf{z}_i = \\sum_{j=1}^{|\\mathbf{w}|} \\mathbf{w}_{j} \\mathbf{x}_{i+j-1}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, suppose $\\mathbf{w} = \\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\end{bmatrix}$ and $\\mathbf{x} = \\begin{bmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix}$. Then $\\mathbf{w} \\star \\mathbf{x} = \\begin{bmatrix}1*1 + 2*2 + 3*3 \\\\ 1*2 + 2*3 + 3*4 \\\\ 1*3 + 2*4 + 3*5 \\end{bmatrix} = \\begin{bmatrix} 14 \\\\ 20 \\\\ 26 \\end{bmatrix}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you reverse either one of $\\mathbf{w}$ or $\\mathbf{x}$, you'll get mathematical convolution.\n",
    "> When machine learning people say \"convolution,\" they usually mean cross-correlation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often refer to $\\mathbf{w}$ as the *kernel*, or the *filter*. Some people use *filter* to refer to a set of multiple kernels in the same layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see whether you follow. Complete the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(x: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :returns: the 1-dimensional cross-correlation of w and x\n",
    "    \"\"\"\n",
    "    z = torch.empty(x.shape[0] - w.shape[0] + 1)\n",
    "    for i in range(z.shape[0]):\n",
    "        ...  # TODO\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor((1, 2, 3))\n",
    "x = torch.tensor((1, 2, 3, 4, 5))\n",
    "torch.equal(conv1d(x, w), torch.tensor([14, 20, 26]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the following cell to see a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(base64.b64decode(b'eltpXSA9IHcgQCB4W2kgOiBpICsgdy5zaGFwZVswXV0=').decode())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply my vertical edge filter, I'll make a kernel like this\n",
    "\n",
    "$\\mathbf{W} = \\begin{bmatrix} -1 & 0 & 1 \\\\-1 & 0 & 1 \\\\-1 & 0 & 1 \\end{bmatrix}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and perform a 2-dimensional convolution over the image. You can probably guess how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    :returns: the 2-dimensional cross-correlation of w and x\n",
    "    \"\"\"\n",
    "    z = torch.empty(tuple(torch.tensor(x.shape) - torch.tensor(w.shape) + 1))\n",
    "    for i in range(z.shape[0]):\n",
    "        for j in range(z.shape[1]):\n",
    "            ...  # TODO\n",
    "    return z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be thinking that this is very inefficient, and you'd be right. For now, let's just use it and leave the performance tweaking to others.\n",
    "\n",
    "We'll compare the output of `conv2d` with the `nn.functional` version of `conv2d`, which is a function you can just call instead of making a nn.Conv2d instance and calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 4)\n",
    "w = torch.randn(2, 2)\n",
    "torch.allclose(torch.nn.functional.conv2d(x[None, :], w[None, None, :])[0], conv2d(x, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(base64.b64decode(b'eltpLCBqXSA9ICh3ICogeFtpIDogaSArIHcuc2hhcGVbMF0sIGogOiBqICsgdy5zaGFwZVsxXV0pLnN1bSgp').decode())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our cat image. Let's apply this filter and look at the vertical edges detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_edges = conv2d(downsampled_image[0], torch.tensor(((-1, 0, 1),) * 3))[None, :]\n",
    "show_images(vertical_edges)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's harder now, but I can still tell this is a cat. Let's save it as JPEG and check the file size. Since pixel values were summed while applying this filter, we have to rescale the result in order to save it as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_edges_image = vertical_edges\n",
    "vertical_edges_image -= vertical_edges_image.min()\n",
    "vertical_edges_image *= 255 / vertical_edges_image.max()\n",
    "vertical_edges_image = vertical_edges_image.to(torch.uint8)\n",
    "torchvision.io.write_jpeg(vertical_edges_image, str(temp_folder / \"cat_vertical_edges.jpg\"), quality=90)\n",
    "(temp_folder / \"cat_vertical_edges.jpg\").stat().st_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran this, I ended up with 65404 bytes, down from the original 3586298, so that's about 1/50 the original size."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap,\n",
    "\n",
    "> We (humans) wanted to perform image classification.\n",
    "\n",
    "> We observed that not all pixels are equally relevant.\n",
    "\n",
    "> We tried reducing the amount of irrelevant information in the image by processing it using various methods, one of which was cross-correlation with a kernel created to highlight vertical edges.\n",
    "\n",
    "> We observed a great reduction in information while still being able to correctly classify the image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we reduced the information so much, we can use multiple kernels to inform our decision. For example, we could use a horizontal edge detector, like this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{W} = \\begin{bmatrix} -1 & -1 & -1 \\\\0 & 0 & 0 \\\\1 & 1 & 1 \\end{bmatrix}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll have both the vertical and horizontal edges to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images((vertical_edges, conv2d(downsampled_image[0], torch.tensor(((-1, -1, -1), (0, 0, 0), (1, 1, 1))))[None, :]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth discussing with your friends:\n",
    "> Why is cross-correlation advantageous over simple linear transformations (as in MLPs) in image-related tasks?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning kernels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you'll benefit from using a GPU if you have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural network layers often have many kernels of different shapes and *strides* (how many pixels to move the kernel over after each $i$ or $j$ step; we've been using a stride of 1 in this notebook). The kernels are learnable, so their elements serve as parameters in the neural network.\n",
    "\n",
    "Each kernel learns to detect a certain feature in the input. If they work well, they'll distill the relevant information from the input. Earlier layers detect simple features such as edges, while later layers detect more sophisticated things such as shapes or faces.\n",
    "\n",
    "\n",
    "In classifiers, the combined outputs of the convolutional layers will inform a final linear layer that performs the classification.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we made a kernel that detected vertical edges.\n",
    "\n",
    "$\\begin{bmatrix} -1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "Let's try to learn this kernel from a random initialization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll learn $\\mathbf{W}$ as parameters of a function $\\hat{f}$ that we will train to detect vertical edges, i.e. $\\hat{f}$ is just a 2-dimensional cross-correlation.\n",
    "\n",
    "Instead of using the `conv2d` we wrote before, we'll use the `nn.Conv2d` `Module` in PyTorch. PyTorch `Module`s are functions that keep track of their parameters and gradients. You can compose a bunch of `Module`s together and call a `backward` function at the end, which adds the derivative of the composition with respect to all parameters.\n",
    "\n",
    "This `nn.Conv2d` constructor makes a 2-dimensional cross-correlation function that can take in the result of multiple kernels and apply multiple kernels. Since we're only interested in applying one kernel to a single channel image, we put 1 in the relevant arguments. For 3-channel color images, we would be 3 input channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.get_default_dtype()\n",
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(3, 3), dtype=dtype, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `Module` has initialized a random kernel. Take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d.weight[0, 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a loss function, we'll use the mean squared error (MSE), which is also available as a `Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a training dataset, we'll use MNIST, which is a bunch of images of handwritten digits. Although this dataset is usually used for image classification, we're only using it here to train this one kernel. The training set contains 60000 grayscale images shaped 28 $\\times$ 28.\n",
    "\n",
    "The first time you run the following cell, the MNIST dataset will be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_set = torchvision.datasets.MNIST(root=\"data\", train=True, download=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nn.Conv2d` module expects inputs of shape (batch size, number of channels, height, width). Since MNIST images are single-channel grayscale, we have to insert an extra dimension of size 1 to be the number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_images = mnist_train_set.data.to(dtype).to(device).unsqueeze(1)\n",
    "mnist_images.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, to avoid blowups, let's scale the pixel values to [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_images /= 255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our own \"labels\" for this dataset by applying the cross-correlation with the reference kernel. To speed things up, I'll use the `nn.functional` version of `conv2d`, which can handle a batch of inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transformed_images = nn.functional.conv2d(\n",
    "    mnist_images, torch.tensor(((-1, -1, -1), (0, 0, 0), (1, 1, 1)), dtype=dtype, device=device)[None, None, :]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example. Below from left to right is\n",
    "* an image in MNIST\n",
    "* what cross-correlation with the randomly initialized $\\mathbf{W}$ does to it\n",
    "* what the reference kernel would do to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.no_grad():\n",
    "    show_images((mnist_images[0].cpu(), conv2d(mnist_images[0][None, :])[0].cpu(), target_transformed_images[0].cpu()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set some hyperparameters,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "N_EPOCHS = 50\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and make a `DataLoader` out of the original images and the desired processed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = data.DataLoader(\n",
    "    data.TensorDataset(mnist_images, target_transformed_images), batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has many `Optimizer`s, which take care of updating parameters and remembering updates to allow more sophisticated update schedules. We'll use `optim.SGD`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(conv2d.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the training loop. Upon loading each batch, \n",
    "1. The optimizer zeros all the gradients.\n",
    "1. The $\\hat{f}$ performs cross-correlation on the batch of images using the current kernel.\n",
    "1. The loss function calculates a scalar loss value.\n",
    "1. We call `backward` on the loss value, which adds the gradient of the loss with respect to each parameter to the gradients.\n",
    "1. We call `step` on the `Optimizer`, which updates the parameters using the gradients and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in (progress_bar := tqdm.tqdm(range(N_EPOCHS), desc=f\"training\")):\n",
    "    epoch_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        z = conv2d(x.to(device))\n",
    "        loss = criterion(z, y.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    progress_bar.set_postfix_str(f\"loss: {epoch_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we've trained for a while. Guess what the kernel is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d.weight[0][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably not very close to the reference kernel. But let's take a look at a cross-correlation using this kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.autograd.no_grad():\n",
    "    show_images((mnist_images[0].cpu(), conv2d(mnist_images[0][None, :])[0].cpu(), target_transformed_images[0].cpu()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, even though the kernel wasn't close, the effect of using it was pretty close."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth discussing with your friends:\n",
    "> Why was the learned kernel not close to the reference kernel used to generate the target data, while the effect of using it was similar."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(base64.b64decode(b'Y29udjJkLndlaWdodFswXVswXS5tZWFuKDEp').decode())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional layers in image classifiers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's classify MNIST images. We will start without any cross-correlation and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerPerceptron(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        \"\"\"\n",
    "        Flatten MNIST -> Linear -> ReLU -> Linear to 10 logits.\n",
    "\n",
    "        ReLU doesn't count as a \"layer\" because it doesn't have learnable parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # MNIST image batches shaped (batch size, 1, 28, 28) will be flattened to\n",
    "        # (batch size, 28 * 28) in the forward method.\n",
    "        self.linear1 = nn.Linear(28 * 28, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear2(self.relu(self.linear1(torch.flatten(x, -3, -1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out a different optimizer. Here is [Adam](https://arxiv.org/abs/1412.6980)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerPerceptron(32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect this two-layer perceptron to have a lot of parameters for the task. Here is how you count them. If you have 32 hidden units, you have\n",
    "* 28 $*$ 28 $*$ 32 for the weight of the first linear layer\n",
    "* 32 for the bias of the first linear layer\n",
    "* 32 $*$ 10 for the second linear layer\n",
    "* 10 for the bias of the second linear layer\n",
    "\n",
    "In total, there are 25450 learnable parameters.\n",
    "\n",
    "PyTorch can count them like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(parameter.numel() for parameter in model.parameters() if parameter.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that MNIST images are provided as images shaped (28, 28), and we had to add a dimension to make them shaped (1, 28, 28) to make them compatible with the `Conv2d` module. Also, pixel values are given as 8-bit integers, and we converted them to floating-point numbers and divided by 255, so that they were all within [0, 1]. `torchvision` gives us a way to do these while loading the data. In the `transform` argument of the dataset constructor, we can specify a transform or a composition of transforms. The one we want is [ToTensor](https://pytorch.org/vision/0.11/transforms.html?highlight=totensor#torchvision.transforms.ToTensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.DataLoader(\n",
    "    torchvision.datasets.MNIST(root=\"data\", train=True, download=True, transform=torchvision.transforms.ToTensor()),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "validation_dataloader = data.DataLoader(\n",
    "    torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=torchvision.transforms.ToTensor()),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are doing classification, the appropriate loss function is cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function trains the module on the MNIST training set, evaluating it on the validation set after each epoch.\n",
    "\n",
    "The forward method of the [`CrossEntropyLoss` module](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) can accept inputs of various shapes. Here, we use it by putting in the batched logits shaped (batch size, number of classes) as the first argument, and the labels shaped (batch size,) as the second argument. The labels are integers from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_model(model: nn.Module):\n",
    "    for epoch_number in range(1, N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        n_correct = 0\n",
    "        n_examples = 0\n",
    "        for examples, labels in (progress_bar := tqdm.tqdm(train_dataloader, desc=f\"training epoch {epoch_number}\")):\n",
    "            optimizer.zero_grad()\n",
    "            examples = examples.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(examples)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            n_correct += (logits.max(1).indices == labels).sum().item()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            n_examples += labels.shape[0]\n",
    "            progress_bar.set_postfix_str(\n",
    "                f\"mean loss: {epoch_loss / n_examples:.5f}, accuracy: {n_correct / n_examples:.4f}\"\n",
    "            )\n",
    "\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        n_correct = 0\n",
    "        n_examples = 0\n",
    "        for examples, labels in (progress_bar := tqdm.tqdm(validation_dataloader, desc=\"validating\")):\n",
    "            examples = examples.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(examples)\n",
    "            loss = criterion(logits, labels)\n",
    "            n_correct += (logits.max(1).indices == labels).sum().item()\n",
    "\n",
    "            validation_loss += loss.item()\n",
    "            n_examples += labels.shape[0]\n",
    "            progress_bar.set_postfix_str(\n",
    "                f\"mean loss: {validation_loss / n_examples:.5f}, accuracy: {n_correct / n_examples:.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_model(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran this, the best validation accuracy was 0.9648 after epoch 5."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth discussing with your friends:\n",
    " > Why does validation accuracy decrease while training accuracy continues to increase?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace the first linear layer with a 2-dimensional convolutional layer. After the ReLU, we will insert a max pooling layer, which collects the maximum elements in small windows distributed throughout its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvolutional(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        convolution_out_channels: int,\n",
    "        convolution_kernel_shape: tuple[int, int],\n",
    "        stride: tuple[int, int] = (1, 1),\n",
    "        max_pool_kernel_shape: tuple[int, int] = (2, 2),\n",
    "        max_pool_stride: tuple[int, int] = (2, 2),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolution = nn.Conv2d(\n",
    "            in_channels=1, out_channels=convolution_out_channels, kernel_size=convolution_kernel_shape\n",
    "        )\n",
    "        output_shape = (torch.tensor((28, 28)) - torch.tensor(convolution_kernel_shape)) // torch.tensor(stride) + 1\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=max_pool_kernel_shape, stride=max_pool_stride)\n",
    "        output_shape = (output_shape - torch.tensor(max_pool_kernel_shape)) // torch.tensor((2, 2)) + 1\n",
    "        self.linear = nn.Linear(convolution_out_channels * torch.prod(output_shape), 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        t = self.convolution(x)\n",
    "        t = self.relu(t)\n",
    "        t = self.max_pool(t)\n",
    "        t = self.linear(torch.flatten(t, -3, -1))\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleConvolutional(2, (3, 3)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "try_model(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained the two-layer perceptron and this simple convolutional model multiple times and they both get about the same validation accuracy, without a consistent winner. But let's look at the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(parameter.numel() for parameter in model.parameters() if parameter.requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Ours is better.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try to improve upon the `SimpleConvolutional` module. There are many directions in which you can improve it, not just increase accuracy. For example, you can maintain validation accuracy while\n",
    "* Decreasing the number of parameters\n",
    "* Decreasing training time or energy usage\n",
    "* Decreasing inference time, energy usage, or memory usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change whatever you want, including the optimizer and hyperparameters. Try to stay above 0.96 validation accuracy. When you are satisfied, prepare to describe your model in English, without code, pseudocode, or framework-specific (PyTorch) terminology, in sufficient detail that a peer could reproduce your experiments from your description. We will ask for your description in a quiz."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
